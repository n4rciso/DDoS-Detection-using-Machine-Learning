{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85323971-e600-49e4-a170-a37843ad3d23",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf6c4c-9e70-4ee2-a97a-7ad82c3d791f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "ddos_data = pd.read_csv(\"ddos_dataset.csv\", sep=\",\")\n",
    "ddos_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "ddos_data['SimillarHTTP'] = ddos_data['SimillarHTTP'].apply(lambda x: str(x) if x == 0 else x)\n",
    "ddos_data['SimillarHTTP'] = ddos_data['SimillarHTTP'].str.strip().fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40dab6-bfe8-427b-9250-1f2c6806873b",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#standardize\n",
    "# Get X and y\n",
    "ddos_data.drop(columns=[\"Unnamed: 0\",\"Destination IP\",\"Source IP\",\"Timestamp\",\"SimillarHTTP\"],axis=1,inplace=True)\n",
    "ddos_data=ddos_data.set_index('Flow ID')\n",
    "X = ddos_data.drop(columns=['label']).to_numpy()\n",
    "y = ddos_data[['label']].values\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_s = scaler.fit_transform(X)\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=9, random_state = 15)\n",
    "X_s = pca.fit_transform(X_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01140e05-da7c-4d5f-adcd-1df49d5188ba",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "#GaussianMixture\n",
    "#In this case use as hyper-parameter init_params = kmeans\n",
    "\n",
    "#We computer the different scores for each iteration\n",
    "\n",
    "n_cluster_list=[]\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "log_l_list=[]\n",
    "\n",
    "#we execute in parallel\n",
    "from joblib import Parallel, delayed\n",
    "def cluster_and_evaluate(n_clusters, X_s, y):\n",
    "    gmm = GaussianMixture(n_components=n_clusters, init_params='kmeans', random_state = 42)\n",
    "    cl_labels = gmm.fit_predict(X_s)\n",
    "    silhouette = silhouette_score(X_s, cl_labels)\n",
    "    ri = rand_score(np.ravel(y), cl_labels)\n",
    "    ari = adjusted_rand_score(np.ravel(y), cl_labels)\n",
    "    log_likelihood = gmm.score(X_s)\n",
    "    return n_clusters, silhouette, ri, ari, log_likelihood\n",
    "\n",
    "# Use Parallel and delayed to parallelize the loop\n",
    "results = Parallel(n_jobs=-1)(delayed(cluster_and_evaluate)(n_clusters, X_s, y) for n_clusters in range(3, 8))\n",
    "\n",
    "# Extract the results into separate lists\n",
    "n_cluster_list, shs_list, ri_list, ari_list, log_l_list = zip(*results)\n",
    "\n",
    "#2. Plot the silhouette score\n",
    "\n",
    "# Get n_clusters leading to the highest silhouette\n",
    "best_sh= np.max(shs_list)\n",
    "best_n=n_cluster_list[np.argmax(shs_list)]\n",
    "print(\"best k: \",best_n, \" with corresponding silhouette: \", best_sh)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot GMM total log-likelihood score\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,log_l_list, marker='o', markersize=5)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('GMM total log-likelihood score')\n",
    "plt.show()\n",
    "\n",
    "# Plot ARI\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,ari_list, marker='o', markersize=5)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca7006-59d2-42bc-86aa-0309696a9f2d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#GaussianMixture\n",
    "#In this case use as hyper-parameter init_params = random\n",
    "\n",
    "#We computer the different scores for each iteration\n",
    "\n",
    "# Function to perform GMM clustering and store metrics\n",
    "def perform_gmm_clustering(n_clusters, X_s, y):\n",
    "    gmm = GaussianMixture(n_components=n_clusters, init_params='random')\n",
    "    cl_labels = gmm.fit_predict(X_s)\n",
    "\n",
    "    silhouette = silhouette_score(X_s, cl_labels)\n",
    "    ri = rand_score(np.ravel(y), cl_labels)\n",
    "    ari = adjusted_rand_score(np.ravel(y), cl_labels)\n",
    "    log_l = gmm.score(X_s)\n",
    "\n",
    "    return n_clusters, silhouette, ri, ari, log_l\n",
    "\n",
    "# Initial broader search\n",
    "n_cluster_list = []\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "log_l_list = []\n",
    "\n",
    "# Define the range for initial search\n",
    "initial_search_range = range(3, 16, 3)  # Larger step size\n",
    "\n",
    "# Use Parallel and delayed to parallelize the initial search\n",
    "results_initial = Parallel(n_jobs=-1)(delayed(perform_gmm_clustering)(n_clusters, X_s, y) for n_clusters in initial_search_range)\n",
    "\n",
    "# Extract the results into separate lists\n",
    "n_cluster_list, shs_list, ri_list, ari_list, log_l_list = zip(*results_initial)\n",
    "\n",
    "# Find the best initial k\n",
    "best_sh = np.max(shs_list)\n",
    "best_n = n_cluster_list[np.argmax(shs_list)]\n",
    "best_ri = ri_list[np.argmax(shs_list)]\n",
    "best_ari = ari_list[np.argmax(shs_list)]\n",
    "print(\"Best refined k: \", best_n, \" with corresponding silhouette: \", best_sh, \" ri: \", best_ri, \" ari: \", best_ari)\n",
    "\n",
    "\n",
    "# Refine search around the best k found\n",
    "refined_n_cluster_list = []\n",
    "refined_shs_list = []\n",
    "refined_ri_list = []\n",
    "refined_ari_list = []\n",
    "refined_log_l_list = []\n",
    "\n",
    "# Define the refined range\n",
    "refined_range = range(max(3, best_n - 2), min(16, best_n + 3))\n",
    "\n",
    "# Use Parallel and delayed to parallelize the loop for the refined search\n",
    "results_refined = Parallel(n_jobs=-1)(delayed(perform_gmm_clustering)(n_clusters, X_s, y) for n_clusters in refined_range)\n",
    "\n",
    "# Extract the results into separate lists\n",
    "refined_n_cluster_list, refined_shs_list, refined_ri_list, refined_ari_list, refined_log_l_list = zip(*results_refined)\n",
    "\n",
    "\n",
    "# Find the best refined k\n",
    "best_refined_sh = np.max(refined_shs_list)\n",
    "best_refined_n = refined_n_cluster_list[np.argmax(refined_shs_list)]\n",
    "best_refined_ri = refined_ri_list[np.argmax(refined_shs_list)]\n",
    "best_refined_ari = refined_ari_list[np.argmax(refined_shs_list)]\n",
    "print(\"Best refined k: \", best_refined_n, \" with corresponding silhouette: \", best_refined_sh, \" ri: \", best_refined_ri, \" ari: \", best_refined_ari, )\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(refined_n_cluster_list,refined_shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot GMM total log-likelihood score\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(refined_n_cluster_list,refined_log_l_list, marker='o', markersize=5)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('GMM total log-likelihood score')\n",
    "plt.show()\n",
    "\n",
    "# Plot ARI\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(refined_n_cluster_list,refined_ari_list, marker='o', markersize=5)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('ARI')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a155ba3-5d38-4030-bfcd-5803bd25f1bf",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#K-means\n",
    "\n",
    "# Function to perform KMeans clustering and store metrics\n",
    "def perform_kmeans_clustering(n_clusters, X_s, y):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    cl_labels = kmeans.fit_predict(X_s)\n",
    "\n",
    "    silhouette = silhouette_score(X_s, cl_labels)\n",
    "    ri = rand_score(np.ravel(y), cl_labels)\n",
    "    ari = adjusted_rand_score(np.ravel(y), cl_labels)\n",
    "    inertia = kmeans.inertia_\n",
    "\n",
    "    return n_clusters, silhouette, ri, ari, inertia\n",
    "\n",
    "# Step 1: Initial broader search\n",
    "n_cluster_list = []\n",
    "shs_list = []\n",
    "ri_list = []\n",
    "ari_list = []\n",
    "inertia_list = []\n",
    "\n",
    "# Define a range of n_clusters for the initial broader search\n",
    "initial_range = range(3, 16, 3)\n",
    "\n",
    "# Use Parallel and delayed to parallelize the loop\n",
    "results = Parallel(n_jobs=-1)(delayed(perform_kmeans_clustering)(n_clusters, X_s, y) for n_clusters in initial_range)\n",
    "\n",
    "# Extract the results into separate lists\n",
    "n_cluster_list, shs_list, ri_list, ari_list, inertia_list = zip(*results)\n",
    "\n",
    "# Get n_clusters leading to the highest silhouette\n",
    "best_sh = np.max(shs_list)\n",
    "best_n = n_cluster_list[np.argmax(shs_list)]\n",
    "print(\"Best initial k: \", best_n, \" with corresponding silhouette: \", best_sh)\n",
    "\n",
    "# Step 2: Refine search around the best k found\n",
    "refined_n_cluster_list = []\n",
    "refined_shs_list = []\n",
    "refined_ri_list = []\n",
    "refined_ari_list = []\n",
    "refined_inertia_list = []\n",
    "\n",
    "# Define the refined range around the best k\n",
    "refined_range = range(max(3, best_n - 2), min(16, best_n + 3))\n",
    "    \n",
    "# Use Parallel and delayed to parallelize the loop for the refined search\n",
    "results_refined = Parallel(n_jobs=-1)(delayed(perform_kmeans_clustering)(n_clusters, X_s, y) for n_clusters in refined_range)\n",
    "\n",
    "# Extract the results into separate lists\n",
    "refined_n_cluster_list, refined_shs_list, refined_ri_list, refined_ari_list, refined_inertia_list = zip(*results_refined)\n",
    "\n",
    "# Get the best refined k\n",
    "best_refined_sh = np.max(refined_shs_list)\n",
    "best_refined_n = refined_n_cluster_list[np.argmax(refined_shs_list)]\n",
    "print(\"Best refined k: \", best_refined_n, \" with corresponding silhouette: \", best_refined_sh)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "plt.plot(n_cluster_list,shs_list, marker='o', markersize=5)\n",
    "plt.scatter(best_n, best_sh, color='r', marker='x', s=90)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0e41f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Utilizziamo quindi il modello KMeans\n",
    "\n",
    "# è stato deciso che 12 è il migliore dal precedente punto.\n",
    "model = KMeans(n_clusters=12)\n",
    "\n",
    "# Ottieni le etichette del cluster\n",
    "cluster_labels = model.fit_predict(X_s)\n",
    "\n",
    "# Calcola le metriche di clustering\n",
    "silhouette = silhouette_score(X_s, cluster_labels)\n",
    "ari = adjusted_rand_score(np.ravel(y), cluster_labels)\n",
    "\n",
    "# Stampa le metriche\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Adjusted Rand Index: {ari}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafb45c-3b86-4097-8620-a462f324e32c",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Analizza il numero di flussi per cluster\n",
    "cluster_df = pd.DataFrame({'Cluster': cluster_labels, 'NumFlows': ddos_data.index}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9221f2f-a3cc-483e-9f7c-f93901b06112",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Plot ECDF per il numero di flussi per cluster\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.ecdfplot(data=cluster_df, x='NumFlows', hue='Cluster',palette='coolwarm')\n",
    "plt.title('ECDF of Number of Flows per Cluster')\n",
    "plt.xlabel('Number of Flows')\n",
    "plt.ylabel('ECDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e97c04-4648-497a-a641-f02ad415d258",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
